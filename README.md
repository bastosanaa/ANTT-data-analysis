# ğŸš† Dashboard ANTT - DeclaraÃ§Ã£o de Rede FerroviÃ¡ria

Acesse o dashboard:
[Deployed application ](https://antt-data-analysis.streamlit.app/)

> **Desafio TÃ©cnico**: Engenharia de Dados e VisualizaÃ§Ã£o Interativa com Python

Um dashboard para anÃ¡lise da infraestrutura ferroviÃ¡ria brasileira, desenvolvido como parte de um desafio tÃ©cnico que avalia habilidades em ETL, modelagem de dados e desenvolvimento de painÃ©is interativos.

[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)](https://www.python.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)](https://streamlit.io/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

**[ğŸ‡ºğŸ‡¸ Read in English](#-english-version)**

---

## ğŸ“‹ Sobre o Desafio

Este projeto foi desenvolvido como resposta a um desafio tÃ©cnico que simula um cenÃ¡rio real de engenharia de dados. O objetivo Ã© demonstrar competÃªncias em:

- **IngestÃ£o de Dados**: ExtraÃ§Ã£o de mÃºltiplas fontes (CSV, Excel)
- **Modelagem Relacional**: EstruturaÃ§Ã£o de banco de dados SQLite com relacionamentos
- **ETL Pipeline**: Processo completo de extraÃ§Ã£o, transformaÃ§Ã£o e carga
- **VisualizaÃ§Ã£o de Dados**: Dashboard interativo com insights acionÃ¡veis
- **Arquitetura de Software**: CÃ³digo modular, escalÃ¡vel e bem documentado

### ğŸ¯ Requisitos TÃ©cnicos Atendidos

âœ… IngestÃ£o de **3+ conjuntos de dados** da DeclaraÃ§Ã£o de Rede (ANTT)  
âœ… Modelagem relacional com **chaves e relacionamentos** coerentes  
âœ… Dashboard com **3+ seÃ§Ãµes interativas** e navegaÃ§Ã£o lateral  
âœ… GrÃ¡ficos interativos usando **Plotly**  
âœ… CÃ³digo estruturado e **documentado**  
âœ… RepositÃ³rio Git com **README completo**  
âœ… AplicaÃ§Ã£o executÃ¡vel com `streamlit run app.py`  

### ğŸ“Š AnÃ¡lises DisponÃ­veis

#### 1ï¸âƒ£ Licenciamento de PÃ¡tios
- Impacto da situaÃ§Ã£o operacional nos tempos de licenciamento
- IdentificaÃ§Ã£o de corredores congestionados
- AnÃ¡lise de dispersÃ£o por linha ferroviÃ¡ria

#### 2ï¸âƒ£ Capacidade de Terminais
- DistribuiÃ§Ã£o de capacidade por tipo de mercadoria
- ClassificaÃ§Ã£o automÃ¡tica por setor (MineraÃ§Ã£o, AgrÃ­cola, ConstruÃ§Ã£o)
- Ranking dos maiores terminais
- VisualizaÃ§Ã£o em treemap e grÃ¡ficos de pizza

#### 3ï¸âƒ£ RelaÃ§Ã£o Carga vs. Velocidade
- Trade-off fÃ­sico entre peso e velocidade autorizada (VMA)
- AnÃ¡lise de velocidade comercial (VMC)
- Auditoria de consistÃªncia de dados (VMC > VMA)
- Ranking de eficiÃªncia operacional por corredor

## ğŸ—ï¸ Arquitetura do Projeto

```
antt-railway-dashboard/
â”‚
â”œâ”€â”€ app.py                          # Entry point do dashboard
â”œâ”€â”€ run_etl.py                      # Entry point do pipeline ETL
â”œâ”€â”€ requirements.txt                # DependÃªncias Python
â”œâ”€â”€ README.md                       # Este arquivo
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ config/                         # ConfiguraÃ§Ãµes centralizadas
â”‚   â”œâ”€â”€ settings.py                # Config do dashboard
â”‚   â”œâ”€â”€ translations.py            # Sistema i18n (PT/EN)
â”‚   â””â”€â”€ etl_config.py              # Config do pipeline ETL
â”‚
â”œâ”€â”€ etl/                            # Pipeline ETL
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extract.py                 # ExtraÃ§Ã£o de dados brutos
â”‚   â”œâ”€â”€ transform.py               # Limpeza e transformaÃ§Ã£o
â”‚   â”œâ”€â”€ load.py                    # Carregamento no SQLite
â”‚   â”œâ”€â”€ validators.py              # ValidaÃ§Ãµes de qualidade
â”‚   â””â”€â”€ utils.py                   # FunÃ§Ãµes auxiliares
â”‚
â”œâ”€â”€ src/                            # MÃ³dulos do Dashboard
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â””â”€â”€ queries.py             # Queries SQL organizadas
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ loader.py              # Carregamento com cache
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ helpers.py             # FunÃ§Ãµes auxiliares
â”‚   â””â”€â”€ pages/
â”‚       â”œâ”€â”€ page_yards.py          # PÃ¡gina: PÃ¡tios
â”‚       â”œâ”€â”€ page_capacity.py       # PÃ¡gina: Capacidade
â”‚       â””â”€â”€ page_speed.py          # PÃ¡gina: Velocidade
â”‚
â”œâ”€â”€ data/                           # Dados brutos (nÃ£o versionados)
â”‚   â”œâ”€â”€ raw/                       # CSV/Excel originais da ANTT
â”‚   â””â”€â”€ processed/                 # Dados processados
â”‚
â””â”€â”€ db/
    â””â”€â”€ data/
        â””â”€â”€ antt.db                # Banco SQLite (gerado pelo ETL)
```

## ğŸš€ Como Executar

### PrÃ©-requisitos
- Python 3.11
- Java JDK (ObrigatÃ³rio para o PySpark processar os dados)
- pip (gerenciador de pacotes Python)

### 1. ConfiguraÃ§Ã£o do Ambiente (Windows)

Antes de instalar as dependÃªncias, Ã© recomendÃ¡vel criar um ambiente virtual isolado. No terminal (PowerShell), execute:

```powershell
# 1.1 Crie o ambiente virtual
python -m venv venv

# 1.2 Ative o ambiente
.\venv\Scripts\activate

# 1.3 Configure o JAVA_HOME (Essencial para o PySpark)
# Nota: Verifique se o caminho abaixo corresponde Ã  sua instalaÃ§Ã£o do Java
$env:JAVA_HOME = "C:\Program Files\Java\jdk-17"
```

### 2. Instale as DependÃªncias
```bash
pip install -r requirements.txt
```

### 3. Execute o Pipeline ETL
```bash
python ./db/etl.py
```
> Este comando irÃ¡:
> - Extrair dados dos arquivos CSV/Excel
> - Limpar e transformar os dados
> - Criar o banco de dados SQLite em `db/data/antt.db`

### 4. Execute o Dashboard
```bash
streamlit run app.py
```

### 5. Acesse no Navegador
```
http://localhost:8501
```

## ğŸ“Š Modelo de Dados

### Tabelas Principais

**Tabelas Fato:**
- `patios` - PÃ¡tios ferroviÃ¡rios (tempo de licenciamento, localizaÃ§Ã£o)
- `terminais` - Terminais de carga (capacidade por mercadoria)
- `trechos_fisicos` - Segmentos de via (carga mÃ¡xima, velocidades)

**Tabelas DimensÃ£o:**
- `dim_linhas` - Linhas/corredores ferroviÃ¡rios
- `dim_mercadorias` - Tipos de mercadorias transportadas

### Relacionamentos
```
patios.id_linha â†’ dim_linhas.id_linha
terminais.id_mercadoria â†’ dim_mercadorias.id_mercadoria
trechos_fisicos.linha â†’ dim_linhas.nome_linha
```

## ğŸ› ï¸ Stack TecnolÃ³gica

| Tecnologia | Uso |
|------------|-----|
| **Python 3.8+** | Linguagem base |
| **Streamlit** | Framework web interativo |
| **Pandas** | ManipulaÃ§Ã£o de dados |
| **SQLite** | Banco de dados relacional |
| **Plotly** | VisualizaÃ§Ãµes interativas |
| **Git** | Controle de versÃ£o |


## ğŸ“– Fonte de Dados

Os dados utilizados sÃ£o provenientes da **DeclaraÃ§Ã£o de Rede 2025** publicada pela ANTT (AgÃªncia Nacional de Transportes Terrestres):

ğŸ”— [Portal ANTT - DeclaraÃ§Ã£o de Rede](https://www.gov.br/antt/pt-br/assuntos/ferrovias/declaracao-de-rede)

### Conjuntos de Dados Utilizados:
1. **PÃ¡tios FerroviÃ¡rios** - CaracterÃ­sticas operacionais e tempos
2. **Terminais de Carga** - Capacidades por tipo de mercadoria
3. **Trechos FÃ­sicos** - EspecificaÃ§Ãµes tÃ©cnicas da via
4. **Linhas FerroviÃ¡rias** - InformaÃ§Ãµes dos corredores
5. **Mercadorias** - ClassificaÃ§Ã£o de cargas

## ğŸ¤ ContribuiÃ§Ãµes

Este Ã© um projeto de portfÃ³lio pessoal desenvolvido como desafio tÃ©cnico. SugestÃµes e feedback sÃ£o bem-vindos!

[â¬†ï¸ Voltar ao topo](#-dashboard-antt---declaraÃ§Ã£o-de-rede-ferroviÃ¡ria)

---
---
---

# ğŸ‡ºğŸ‡¸ English Version

# ğŸš† ANTT Dashboard - Railway Network Declaration

Access the dashboard
[Deployed application ](https://antt-data-analysis.streamlit.app/)


> **Technical Challenge**: Data Engineering and Interactive Visualization with Python

A dashboard for analyzing Brazilian railway infrastructure, developed as part of a technical challenge that evaluates skills in ETL, data modeling, and interactive panel development.

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)](https://streamlit.io/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

**[ğŸ‡§ğŸ‡· Ler em PortuguÃªs](#-dashboard-antt---declaraÃ§Ã£o-de-rede-ferroviÃ¡ria)**

---

## ğŸ“‹ About the Challenge

This project was developed in response to a technical challenge that simulates a real data engineering scenario. The goal is to demonstrate competencies in:

- **Data Ingestion**: Extraction from multiple sources (CSV, Excel)
- **Relational Modeling**: SQLite database structuring with relationships
- **ETL Pipeline**: Complete extraction, transformation, and loading process
- **Data Visualization**: Interactive dashboard with actionable insights
- **Software Architecture**: Modular, scalable, and well-documented code

### ğŸ¯ Technical Requirements Met

âœ… Ingestion of **3+ datasets** from Network Declaration (ANTT)  
âœ… Relational modeling with coherent **keys and relationships**  
âœ… Dashboard with **3+ interactive sections** and sidebar navigation  
âœ… Interactive charts using **Plotly**  
âœ… Structured and **documented** code  
âœ… Git repository with **complete README**  
âœ… Executable application with `streamlit run app.py`  

### ğŸ“Š Available Analyses

#### 1ï¸âƒ£ Yard Licensing
- Impact of operational status on licensing times
- Identification of congested corridors
- Distribution analysis by railway line

#### 2ï¸âƒ£ Terminal Capacity
- Capacity distribution by commodity type
- Automatic classification by sector (Mining, Agriculture, Construction)
- Ranking of largest terminals
- Treemap and pie chart visualization

#### 3ï¸âƒ£ Load vs. Speed Relationship
- Physical trade-off between weight and authorized speed (VMA)
- Commercial speed analysis (VMC)
- Data consistency audit (VMC > VMA)
- Operational efficiency ranking by corridor

## ğŸ—ï¸ Project Architecture

```
antt-railway-dashboard/
â”‚
â”œâ”€â”€ app.py                          # Dashboard entry point
â”œâ”€â”€ run_etl.py                      # ETL pipeline entry point
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ config/                         # Centralized configuration
â”‚   â”œâ”€â”€ settings.py                # Dashboard config
â”‚   â”œâ”€â”€ translations.py            # i18n system (PT/EN)
â”‚   â””â”€â”€ etl_config.py              # ETL pipeline config
â”‚
â”œâ”€â”€ etl/                            # ETL Pipeline
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extract.py                 # Raw data extraction
â”‚   â”œâ”€â”€ transform.py               # Cleaning and transformation
â”‚   â”œâ”€â”€ load.py                    # Loading into SQLite
â”‚   â”œâ”€â”€ validators.py              # Quality validations
â”‚   â””â”€â”€ utils.py                   # Helper functions
â”‚
â”œâ”€â”€ src/                            # Dashboard Modules
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â””â”€â”€ queries.py             # Organized SQL queries
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ loader.py              # Loading with cache
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ helpers.py             # Helper functions
â”‚   â””â”€â”€ pages/
â”‚       â”œâ”€â”€ page_yards.py          # Page: Yards
â”‚       â”œâ”€â”€ page_capacity.py       # Page: Capacity
â”‚       â””â”€â”€ page_speed.py          # Page: Speed
â”‚
â”œâ”€â”€ data/                           # Raw data (not versioned)
â”‚   â”œâ”€â”€ raw/                       # Original CSV/Excel from ANTT
â”‚   â””â”€â”€ processed/                 # Processed data
â”‚
â””â”€â”€ db/
    â””â”€â”€ data/
        â””â”€â”€ antt.db                # SQLite database (generated by ETL)
```

## ğŸš€ How to Run

### Prerequisites
- Python 3.11
- Java JDK (for PySpark)
- pip (Python package manager)

### 1. Virtual Environment (Windows)

```powershell
# 1.1 Create venv
python -m venv venv

# 1.2 Activate environment
.\venv\Scripts\activate

# 1.3 Configure JAVA_HOME 
# Note: Check the path of your own Java instalation
$env:JAVA_HOME = "C:\Program Files\Java\jdk-17"
```


### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Run ETL Pipeline
```bash
python ./db/etl.py
```
> This command will:
> - Extract data from CSV/Excel files
> - Clean and transform the data
> - Create SQLite database in `db/data/antt.db`

### 4. Run Dashboard
```bash
streamlit run app.py
```

### 5. Access in Browser
```
http://localhost:8501
```

## ğŸ“Š Data Model

### Main Tables

**Fact Tables:**
- `patios` - Railway yards (licensing time, location)
- `terminais` - Cargo terminals (capacity by commodity)
- `trechos_fisicos` - Track segments (max load, speeds)

**Dimension Tables:**
- `dim_linhas` - Railway lines/corridors
- `dim_mercadorias` - Types of transported commodities

### Relationships
```
patios.id_linha â†’ dim_linhas.id_linha
terminais.id_mercadoria â†’ dim_mercadorias.id_mercadoria
trechos_fisicos.linha â†’ dim_linhas.nome_linha
```

## ğŸ› ï¸ Technology Stack

| Technology | Usage |
|------------|-------|
| **Python 3.8+** | Base language |
| **Streamlit** | Interactive web framework |
| **Pandas** | Data manipulation |
| **SQLite** | Relational database |
| **Plotly** | Interactive visualizations |
| **Git** | Version control |


## ğŸ“– Data Source

The data used is from the **2025 Network Declaration** published by ANTT (National Land Transportation Agency):

ğŸ”— [ANTT Portal - Network Declaration](https://www.gov.br/antt/pt-br/assuntos/ferrovias/declaracao-de-rede)

### Datasets Used:
1. **Railway Yards** - Operational characteristics and times
2. **Cargo Terminals** - Capacities by commodity type
3. **Physical Segments** - Technical track specifications
4. **Railway Lines** - Corridor information
5. **Commodities** - Cargo classification

## ğŸ¤ Contributions

This is a personal portfolio project developed as a technical challenge. Suggestions and feedback are welcome!

[â¬†ï¸ Back to top](#-english-version)
